{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_path = 'C:/Python_Repository/ICT_language_processing/katalkhae/data/ìˆ˜ì§„ ì¹´ì¹´ì˜¤í†¡ ëŒ€í™”ë‚´ìš©.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:/Python_Repository/ICT_language_processing/katalkhae/data/ìˆ˜ì§„ ì¹´ì¹´ì˜¤í†¡ ëŒ€í™”ë‚´ìš©.txt'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "katalk_data = list()\n",
    "katalk_msg_pattern = \"[0-9]{4}[ë…„.] [0-9]{1,2}[ì›”.] [0-9]{1,2}[ì¼.] ì˜¤\\S [0-9]{1,2}:[0-9]{1,2},.*:\"\n",
    "data_info = \"[0-9]{4}ë…„ [0-9]{1,2}ì›” [0-9]{1,2}ì¼ \\Sìš”ì¼\" #ë‚ ì§œë§Œ ìˆëŠ” ì¤„ì„ ê°€ë ¤ë‚´ì–´ ë¬´ì‹œí•˜ê¸° ìœ„í•œ ì •ê·œì‹í‘œí˜„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def katalk_msg_parse(file_path):\n",
    "    katalk_data = list()\n",
    "    katalk_msg_pattern = \"[0-9]{4}[ë…„.] [0-9]{1,2}[ì›”.] [0-9]{1,2}[ì¼.] ì˜¤\\S [0-9]{1,2}:[0-9]{1,2},.*:\"\n",
    "    data_info = \"[0-9]{4}ë…„ [0-9]{1,2}ì›” [0-9]{1,2}ì¼ \\Sìš”ì¼\" #ë‚ ì§œë§Œ ìˆëŠ” ì¤„ì„ ê°€ë ¤ë‚´ì–´ ë¬´ì‹œí•˜ê¸° ìœ„í•œ ì •ê·œì‹í‘œí˜„\n",
    "\n",
    "    for line in open(file_path, encoding='utf-8'):\n",
    "        if(re.match(data_info, line)) or (line==''): # í˜„ì¬ ì½ì€ ì¤„ì´ ë‚ ì§œë§Œ ìˆê±°ë‚˜, ë¹ˆ ì¤„ì´ë©´ continue\n",
    "            continue\n",
    "        elif re.match(katalk_msg_pattern, line): # ì¹´í†¡ ë©”ì‹œì§€ íŒ¨í„´ê³¼ ì¼ì¹˜í•˜ë©´\n",
    "            line = line.split(\",\") # ì‰¼í‘œë¥¼ ê¸°ì¤€ìœ¼ë¡œ í…ìŠ¤íŠ¸ë¥¼ ë‚˜ëˆ”\n",
    "            date_time = line[0] # ì‰¼í‘œì˜ ì•ë¶€ë¶„ì€ ë‚ ì§œ\n",
    "            user_text = line[1].split(\" : \", maxsplit=1)\n",
    "            user_name = user_text[0].strip()\n",
    "            text = user_text[1].strip()\n",
    "            katalk_data.append({'datetime': date_time,\n",
    "                        'user_name': user_name,\n",
    "                        'text': text\n",
    "            })\n",
    "        else:\n",
    "           if len(katalk_data) > 0: # ë§Œì•½ í˜„ì¬ ì½ì€ ì¤„ì´ ë‚ ì§œë„, ë¹ˆ ì¤„ë„, ë©”ì‹œì§€ íŒ¨í„´ë„ ì•„ë‹ˆë¼ë©´ ë©”ì‹œì§€ë¥¼ ì´ì „ ë¼ì¸ì— ì´ì–´ë¶™ì„\n",
    "               katalk_data[-1]['text'] += \"\\n\"+line.strip()\n",
    "\n",
    "    return katalk_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = katalk_msg_parse(f_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>user_name</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019ë…„ 9ì›” 3ì¼ ì˜¤í›„ 6:14</td>\n",
       "      <td>ìˆ˜ì§„</td>\n",
       "      <td>ì•ˆë…•í•˜ì„¸ìš” ì„±ì¸ì „ë¬¸ ë ˆìŠ¤ë„ˆ ìˆ˜ì§„ìŒ¤ì…ë‹ˆë‹¤ğŸ˜Š\\nì €ëŠ” ìŒëŒ€ í”¼ì•„ë…¸4ë…„ì œ ì¡¸ì—…, ë‹¤ìˆ˜ì˜ ì½©...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019ë…„ 9ì›” 3ì¼ ì˜¤í›„ 6:15</td>\n",
       "      <td>ìˆ˜ì§„</td>\n",
       "      <td>https://m.cafe.naver.com/ArticleSearchList.nhn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019ë…„ 9ì›” 3ì¼ ì˜¤í›„ 6:20</td>\n",
       "      <td>mean</td>\n",
       "      <td>ì•ˆë…•í•˜ì„¸ìš” ì „í™”ë¡œ í˜¹ì‹œ ë¬¸ì˜ê°€ëŠ¥í•œê°€ìš”?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019ë…„ 9ì›” 3ì¼ ì˜¤í›„ 6:23</td>\n",
       "      <td>ìˆ˜ì§„</td>\n",
       "      <td>ë„¤! ì œê°€ ê³§ ìˆ˜ì—…ì´ ìˆì–´ì„œ í˜¹ì‹œ 8ì‹œ 20ë¶„ì— í†µí™” ê°€ëŠ¥í•˜ì‹¤ê¹Œìš”~?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019ë…„ 9ì›” 3ì¼ ì˜¤í›„ 6:23</td>\n",
       "      <td>mean</td>\n",
       "      <td>ë„¤ë„¤ ê·¸ë•Œ ì—°ë½ë¶€íƒë“œë¦½ë‹ˆë‹¤~</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3842</th>\n",
       "      <td>2021ë…„ 9ì›” 27ì¼ ì˜¤í›„ 10:53</td>\n",
       "      <td>ìˆ˜ì§„</td>\n",
       "      <td>íìœ¼ìŒ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3843</th>\n",
       "      <td>2021ë…„ 9ì›” 27ì¼ ì˜¤í›„ 10:53</td>\n",
       "      <td>ìˆ˜ì§„</td>\n",
       "      <td>ë‹´ì£¼ì— ìƒˆë¡œ ê·¼ë¬´í•´ì•¼ë˜ì„± ã… ã…  ì•„ì§ ìŠ¤ì¼€ì¤„ì´ì•ˆë‚˜ì™€ì¨ìš”</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3844</th>\n",
       "      <td>2021ë…„ 9ì›” 27ì¼ ì˜¤í›„ 11:05</td>\n",
       "      <td>mean</td>\n",
       "      <td>ì „ ì›”í†  ë¹¼ê³ ë¼ìš”ã…‹ã…‹ã…‹\\n\\n2021ë…„ 9ì›” 28ì¼ ì˜¤ì „ 12:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3845</th>\n",
       "      <td>2021ë…„ 9ì›” 28ì¼ ì˜¤ì „ 12:36</td>\n",
       "      <td>ìˆ˜ì§„</td>\n",
       "      <td>ìŠ¤ì¼€ì¤„ ëª©ìš”ì¼ì— ë‚˜ì™€ì„œ ã… ã… ë‚˜ì˜¤ëŠ”ëŒ€ë£¨ ì—°ë½í• ê²Œìš¥</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3846</th>\n",
       "      <td>2021ë…„ 9ì›” 28ì¼ ì˜¤ì „ 10:18</td>\n",
       "      <td>mean</td>\n",
       "      <td>ë„¤ì—ã…‹ã…‹ã…‹</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3847 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   datetime user_name  \\\n",
       "0       2019ë…„ 9ì›” 3ì¼ ì˜¤í›„ 6:14        ìˆ˜ì§„   \n",
       "1       2019ë…„ 9ì›” 3ì¼ ì˜¤í›„ 6:15        ìˆ˜ì§„   \n",
       "2       2019ë…„ 9ì›” 3ì¼ ì˜¤í›„ 6:20      mean   \n",
       "3       2019ë…„ 9ì›” 3ì¼ ì˜¤í›„ 6:23        ìˆ˜ì§„   \n",
       "4       2019ë…„ 9ì›” 3ì¼ ì˜¤í›„ 6:23      mean   \n",
       "...                     ...       ...   \n",
       "3842  2021ë…„ 9ì›” 27ì¼ ì˜¤í›„ 10:53        ìˆ˜ì§„   \n",
       "3843  2021ë…„ 9ì›” 27ì¼ ì˜¤í›„ 10:53        ìˆ˜ì§„   \n",
       "3844  2021ë…„ 9ì›” 27ì¼ ì˜¤í›„ 11:05      mean   \n",
       "3845  2021ë…„ 9ì›” 28ì¼ ì˜¤ì „ 12:36        ìˆ˜ì§„   \n",
       "3846  2021ë…„ 9ì›” 28ì¼ ì˜¤ì „ 10:18      mean   \n",
       "\n",
       "                                                   text  \n",
       "0     ì•ˆë…•í•˜ì„¸ìš” ì„±ì¸ì „ë¬¸ ë ˆìŠ¤ë„ˆ ìˆ˜ì§„ìŒ¤ì…ë‹ˆë‹¤ğŸ˜Š\\nì €ëŠ” ìŒëŒ€ í”¼ì•„ë…¸4ë…„ì œ ì¡¸ì—…, ë‹¤ìˆ˜ì˜ ì½©...  \n",
       "1     https://m.cafe.naver.com/ArticleSearchList.nhn...  \n",
       "2                                 ì•ˆë…•í•˜ì„¸ìš” ì „í™”ë¡œ í˜¹ì‹œ ë¬¸ì˜ê°€ëŠ¥í•œê°€ìš”?  \n",
       "3                ë„¤! ì œê°€ ê³§ ìˆ˜ì—…ì´ ìˆì–´ì„œ í˜¹ì‹œ 8ì‹œ 20ë¶„ì— í†µí™” ê°€ëŠ¥í•˜ì‹¤ê¹Œìš”~?  \n",
       "4                                       ë„¤ë„¤ ê·¸ë•Œ ì—°ë½ë¶€íƒë“œë¦½ë‹ˆë‹¤~  \n",
       "...                                                 ...  \n",
       "3842                                                íìœ¼ìŒ  \n",
       "3843                      ë‹´ì£¼ì— ìƒˆë¡œ ê·¼ë¬´í•´ì•¼ë˜ì„± ã… ã…  ì•„ì§ ìŠ¤ì¼€ì¤„ì´ì•ˆë‚˜ì™€ì¨ìš”  \n",
       "3844              ì „ ì›”í†  ë¹¼ê³ ë¼ìš”ã…‹ã…‹ã…‹\\n\\n2021ë…„ 9ì›” 28ì¼ ì˜¤ì „ 12:36  \n",
       "3845                         ìŠ¤ì¼€ì¤„ ëª©ìš”ì¼ì— ë‚˜ì™€ì„œ ã… ã… ë‚˜ì˜¤ëŠ”ëŒ€ë£¨ ì—°ë½í• ê²Œìš¥  \n",
       "3846                                              ë„¤ì—ã…‹ã…‹ã…‹  \n",
       "\n",
       "[3847 rows x 3 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import torch\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150000, 3)\n",
      "(50000, 3)\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(\"C:/Python_Repository/ICT_language_processing/katalkhae/data/nsmc-master/nsmc-master/ratings_train.txt\", sep='\\t')\n",
    "test = pd.read_csv(\"C:/Python_Repository/ICT_language_processing/katalkhae/data/nsmc-master/nsmc-master/ratings_test.txt\", sep='\\t')\n",
    "\n",
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì „ì²˜ë¦¬ - í›ˆë ¨ì…‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                  ì•„ ë”ë¹™.. ì§„ì§œ ì§œì¦ë‚˜ë„¤ìš” ëª©ì†Œë¦¬\n",
       "1                    í ...í¬ìŠ¤í„°ë³´ê³  ì´ˆë”©ì˜í™”ì¤„....ì˜¤ë²„ì—°ê¸°ì¡°ì°¨ ê°€ë³ì§€ ì•Šêµ¬ë‚˜\n",
       "2                                    ë„ˆë¬´ì¬ë°“ì—ˆë‹¤ê·¸ë˜ì„œë³´ëŠ”ê²ƒì„ì¶”ì²œí•œë‹¤\n",
       "3                        êµë„ì†Œ ì´ì•¼ê¸°êµ¬ë¨¼ ..ì†”ì§íˆ ì¬ë¯¸ëŠ” ì—†ë‹¤..í‰ì  ì¡°ì •\n",
       "4    ì‚¬ì´ëª¬í˜ê·¸ì˜ ìµì‚´ìŠ¤ëŸ° ì—°ê¸°ê°€ ë‹ë³´ì˜€ë˜ ì˜í™”!ìŠ¤íŒŒì´ë”ë§¨ì—ì„œ ëŠ™ì–´ë³´ì´ê¸°ë§Œ í–ˆë˜ ì»¤ìŠ¤í‹´ ...\n",
       "5        ë§‰ ê±¸ìŒë§ˆ ë—€ 3ì„¸ë¶€í„° ì´ˆë“±í•™êµ 1í•™ë…„ìƒì¸ 8ì‚´ìš©ì˜í™”.ã…‹ã…‹ã…‹...ë³„ë°˜ê°œë„ ì•„ê¹Œì›€.\n",
       "6                                ì›ì‘ì˜ ê¸´ì¥ê°ì„ ì œëŒ€ë¡œ ì‚´ë ¤ë‚´ì§€ëª»í–ˆë‹¤.\n",
       "7    ë³„ ë°˜ê°œë„ ì•„ê¹ë‹¤ ìš•ë‚˜ì˜¨ë‹¤ ì´ì‘ê²½ ê¸¸ìš©ìš° ì—°ê¸°ìƒí™œì´ëª‡ë…„ì¸ì§€..ì •ë§ ë°œë¡œí•´ë„ ê·¸ê²ƒë³´ë‹¨...\n",
       "8                               ì•¡ì…˜ì´ ì—†ëŠ”ë°ë„ ì¬ë¯¸ ìˆëŠ” ëª‡ì•ˆë˜ëŠ” ì˜í™”\n",
       "9        ì™œì¼€ í‰ì ì´ ë‚®ì€ê±´ë°? ê½¤ ë³¼ë§Œí•œë°.. í—ë¦¬ìš°ë“œì‹ í™”ë ¤í•¨ì—ë§Œ ë„ˆë¬´ ê¸¸ë“¤ì—¬ì ¸ ìˆë‚˜?\n",
       "Name: document, dtype: object"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = train['document']\n",
    "sentences[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS] ì•„ ë”ë¹™.. ì§„ì§œ ì§œì¦ë‚˜ë„¤ìš” ëª©ì†Œë¦¬ [SEP]',\n",
       " '[CLS] í ...í¬ìŠ¤í„°ë³´ê³  ì´ˆë”©ì˜í™”ì¤„....ì˜¤ë²„ì—°ê¸°ì¡°ì°¨ ê°€ë³ì§€ ì•Šêµ¬ë‚˜ [SEP]',\n",
       " '[CLS] ë„ˆë¬´ì¬ë°“ì—ˆë‹¤ê·¸ë˜ì„œë³´ëŠ”ê²ƒì„ì¶”ì²œí•œë‹¤ [SEP]',\n",
       " '[CLS] êµë„ì†Œ ì´ì•¼ê¸°êµ¬ë¨¼ ..ì†”ì§íˆ ì¬ë¯¸ëŠ” ì—†ë‹¤..í‰ì  ì¡°ì • [SEP]',\n",
       " '[CLS] ì‚¬ì´ëª¬í˜ê·¸ì˜ ìµì‚´ìŠ¤ëŸ° ì—°ê¸°ê°€ ë‹ë³´ì˜€ë˜ ì˜í™”!ìŠ¤íŒŒì´ë”ë§¨ì—ì„œ ëŠ™ì–´ë³´ì´ê¸°ë§Œ í–ˆë˜ ì»¤ìŠ¤í‹´ ë˜ìŠ¤íŠ¸ê°€ ë„ˆë¬´ë‚˜ë„ ì´ë»ë³´ì˜€ë‹¤ [SEP]',\n",
       " '[CLS] ë§‰ ê±¸ìŒë§ˆ ë—€ 3ì„¸ë¶€í„° ì´ˆë“±í•™êµ 1í•™ë…„ìƒì¸ 8ì‚´ìš©ì˜í™”.ã…‹ã…‹ã…‹...ë³„ë°˜ê°œë„ ì•„ê¹Œì›€. [SEP]',\n",
       " '[CLS] ì›ì‘ì˜ ê¸´ì¥ê°ì„ ì œëŒ€ë¡œ ì‚´ë ¤ë‚´ì§€ëª»í–ˆë‹¤. [SEP]',\n",
       " '[CLS] ë³„ ë°˜ê°œë„ ì•„ê¹ë‹¤ ìš•ë‚˜ì˜¨ë‹¤ ì´ì‘ê²½ ê¸¸ìš©ìš° ì—°ê¸°ìƒí™œì´ëª‡ë…„ì¸ì§€..ì •ë§ ë°œë¡œí•´ë„ ê·¸ê²ƒë³´ë‹¨ ë‚«ê²Ÿë‹¤ ë‚©ì¹˜.ê°ê¸ˆë§Œë°˜ë³µë°˜ë³µ..ì´ë“œë¼ë§ˆëŠ” ê°€ì¡±ë„ì—†ë‹¤ ì—°ê¸°ëª»í•˜ëŠ”ì‚¬ëŒë§Œëª¨ì—¿ë„¤ [SEP]',\n",
       " '[CLS] ì•¡ì…˜ì´ ì—†ëŠ”ë°ë„ ì¬ë¯¸ ìˆëŠ” ëª‡ì•ˆë˜ëŠ” ì˜í™” [SEP]',\n",
       " '[CLS] ì™œì¼€ í‰ì ì´ ë‚®ì€ê±´ë°? ê½¤ ë³¼ë§Œí•œë°.. í—ë¦¬ìš°ë“œì‹ í™”ë ¤í•¨ì—ë§Œ ë„ˆë¬´ ê¸¸ë“¤ì—¬ì ¸ ìˆë‚˜? [SEP]']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BERT ì…ë ¥ í˜•ì‹ì— ë§ê²Œ ë³€í™˜\n",
    "sentences = [\"[CLS] \" + str(sentences) + \" [SEP]\" for sentences in sentences]\n",
    "sentences[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, ..., 0, 1, 0], dtype=int64)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ë¼ë²¨ ì¶”ì¶œ\n",
    "labels = train['label'].values\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 972k/972k [00:23<00:00, 42.5kB/s]\n",
      "Downloading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29.0/29.0 [00:00<00:00, 9.69kB/s]\n",
      "Downloading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.87M/1.87M [00:30<00:00, 64.4kB/s]\n",
      "Downloading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 625/625 [00:00<00:00, 314kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] ì•„ ë”ë¹™.. ì§„ì§œ ì§œì¦ë‚˜ë„¤ìš” ëª©ì†Œë¦¬ [SEP]\n",
      "['[CLS]', 'ì•„', 'ë”', '##ë¹™', '.', '.', 'ì§„', '##ì§œ', 'ì§œ', '##ì¦', '##ë‚˜', '##ë„¤', '##ìš”', 'ëª©', '##ì†Œ', '##ë¦¬', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "# BERTì˜ í† í¬ë‚˜ì´ì €ë¡œ ë¬¸ì¥ì„ í† í°ìœ¼ë¡œ ë¶„ë¦¬, BERTëŠ” í˜•íƒœì†Œ ë¶„ì„ìœ¼ë¡œ í† í°ì„ ë¶„ë¦¬í•˜ì§€ ì•ŠìŒ. WordPieceë¼ëŠ” í†µê³„ì ì¸ ë°©ì‹ ì‚¬ìš©\n",
    "# í•œ ë‹¨ì–´ë‚´ì—ì„œ ìì£¼ ë‚˜ì˜¤ëŠ” ê¸€ìë“¤ì„ ë¶™ì—¬ í•˜ë‚˜ì˜ í† í°ìœ¼ë¡œ ë§Œë“¦\n",
    "# --> ì–¸ì–´ì— ìƒê´€ì—†ì´ í† í° ìƒì„±, ì‹ ì¡°ì–´ë„ í† í¬ë‚˜ì´ì§• ê°€ëŠ¥\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased', do_lower_case=False)\n",
    "tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
    "\n",
    "print(sentences[0])\n",
    "print(tokenized_texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   101,   9519,   9074, 119005,    119,    119,   9708, 119235,\n",
       "         9715, 119230,  16439,  77884,  48549,   9284,  22333,  12692,\n",
       "          102,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ì…ë ¥í† í°ì˜ ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´\n",
    "MAX_LEN = 128\n",
    "\n",
    "# í† í°ì„ ìˆ«ì ì¸ë±ìŠ¤ë¡œ ë³€í™˜\n",
    "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
    "\n",
    "# ë¬¸ì¥ì„ MAX_LEN ê¸¸ì´ì— ë§ê²Œ ìë¥´ê³ , ëª¨ìë€ ë¶€ë¶„ì„ íŒ¨ë”© 0ìœ¼ë¡œ ì±„ì›€\n",
    "input_ids = pad_sequences(input_ids,  maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "\n",
    "input_ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "# ì–´í…ì…˜ ë§ˆìŠ¤í¬ ì´ˆê¸°í™”\n",
    "attention_masks = []\n",
    "\n",
    "# ì–´í…ì…˜ ë§ˆìŠ¤í¬ë¥¼ íŒ¨ë”©ì´ ì•„ë‹ˆë©´ 1, íŒ¨ë”©ì´ë©´ 0ìœ¼ë¡œ ì„¤ì •\n",
    "# íŒ¨ë”©ë¶€ë¶„ì€ BERT ëª¨ë¸ì—ì„œ ì–´í…ì…˜ì„ ìˆ˜í–‰í•˜ì§€ ì•Šì•„ ì†ë„ í–¥ìƒ\n",
    "for seq in input_ids:\n",
    "    seq_mask = [float(i>0) for i in seq]\n",
    "    attention_masks.append(seq_mask)\n",
    "\n",
    "print(attention_masks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í›ˆë ¨ì…‹ê³¼ ê²€ì¦ì…‹ìœ¼ë¡œ ë¶„ë¦¬\n",
    "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids,\n",
    "                                                                                    labels, \n",
    "                                                                                    random_state=2018, \n",
    "                                                                                    test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì–´í…ì…˜ ë§ˆìŠ¤í¬ë¥¼ í›ˆë ¨ì…‹ê³¼ ê²€ì¦ì…‹ìœ¼ë¡œ ë¶„ë¦¬\n",
    "train_masks, validation_masks, _, _ = train_test_split(attention_masks, \n",
    "                                                       input_ids,\n",
    "                                                       random_state=2018, \n",
    "                                                       test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([   101,   9711,  11489,   9364,  41850,   9004,  32537,   9491,  35506,\n",
      "         17360,  48549,    119,    119,   9477,  26444,  12692,   9665,  21789,\n",
      "         11287,   9708, 119235,   9659,  22458, 119136,  12965,  48549,    119,\n",
      "           119,   9532,  22879,   9685,  16985,  14523,  48549,    119,    119,\n",
      "          9596, 118728,    119,    119,   9178, 106065, 118916,    119,    119,\n",
      "          8903,  11664,  11513,   9960,  14423,  25503, 118671,  48549,    119,\n",
      "           119,  21890,   9546,  37819,  22879,   9356,  14867,   9715, 119230,\n",
      "        118716,  48345,    119,   9663,  23321,  10954,   9638,  35506, 106320,\n",
      "         10739,  20173,   9359,  19105,  11102,  42428,  17196,  48549,    119,\n",
      "           119,    100,    117,   9947,  12945,   9532,  25503,   8932,  14423,\n",
      "         35506, 119050,  11903,  14867,  10003,  14863,  33188,  48345,    119,\n",
      "           102,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0], dtype=torch.int32)\n",
      "tensor(0)\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.])\n",
      "tensor([   101,   1871, 111754, 111754, 111754, 111754,    102,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0], dtype=torch.int32)\n",
      "tensor(1)\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.])\n"
     ]
    }
   ],
   "source": [
    "# ë°ì´í„°ë¥¼ íŒŒì´í† ì¹˜ì˜ í…ì„œë¡œ ë³€í™˜\n",
    "train_inputs = torch.tensor(train_inputs)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "train_masks = torch.tensor(train_masks)\n",
    "validation_inputs = torch.tensor(validation_inputs)\n",
    "validation_labels = torch.tensor(validation_labels)\n",
    "validation_masks = torch.tensor(validation_masks)\t\t\t\t\n",
    "\n",
    "print(train_inputs[0])\n",
    "print(train_labels[0])\n",
    "print(train_masks[0])\n",
    "print(validation_inputs[0])\n",
    "print(validation_labels[0])\n",
    "print(validation_masks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "# íŒŒì´í† ì¹˜ì˜ DataLoaderë¡œ ì…ë ¥, ë§ˆìŠ¤í¬, ë¼ë²¨ì„ ë¬¶ì–´ ë°ì´í„° ì„¤ì •\n",
    "# í•™ìŠµì‹œ ë°°ì¹˜ ì‚¬ì´ì¦ˆ ë§Œí¼ì˜ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜®\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì „ì²˜ë¦¬ - í…ŒìŠ¤íŠ¸ì…‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                                  êµ³ ã…‹\n",
       "1                                 GDNTOPCLASSINTHECLUB\n",
       "2               ë­ì•¼ ì´ í‰ì ë“¤ì€.... ë‚˜ì˜ì§„ ì•Šì§€ë§Œ 10ì  ì§œë¦¬ëŠ” ë”ë”ìš± ì•„ë‹ˆì–ì•„\n",
       "3                     ì§€ë£¨í•˜ì§€ëŠ” ì•Šì€ë° ì™„ì „ ë§‰ì¥ì„... ëˆì£¼ê³  ë³´ê¸°ì—ëŠ”....\n",
       "4    3Dë§Œ ì•„ë‹ˆì—ˆì–´ë„ ë³„ ë‹¤ì„¯ ê°œ ì¤¬ì„í…ë°.. ì™œ 3Dë¡œ ë‚˜ì™€ì„œ ì œ ì‹¬ê¸°ë¥¼ ë¶ˆí¸í•˜ê²Œ í•˜ì£ ??\n",
       "5                                   ìŒì•…ì´ ì£¼ê°€ ëœ, ìµœê³ ì˜ ìŒì•…ì˜í™”\n",
       "6                                              ì§„ì •í•œ ì“°ë ˆê¸°\n",
       "7             ë§ˆì¹˜ ë¯¸êµ­ì• ë‹ˆì—ì„œ íŠ€ì–´ë‚˜ì˜¨ë“¯í•œ ì°½ì˜ë ¥ì—†ëŠ” ë¡œë´‡ë””ìì¸ë¶€í„°ê°€,ê³ ê°œë¥¼ ì –ê²Œí•œë‹¤\n",
       "8    ê°ˆìˆ˜ë¡ ê°œíŒë˜ê°€ëŠ” ì¤‘êµ­ì˜í™” ìœ ì¹˜í•˜ê³  ë‚´ìš©ì—†ìŒ í¼ì¡ë‹¤ ëë‚¨ ë§ë„ì•ˆë˜ëŠ” ë¬´ê¸°ì— ìœ ì¹˜í•œc...\n",
       "9       ì´ë³„ì˜ ì•„í””ë’¤ì— ì°¾ì•„ì˜¤ëŠ” ìƒˆë¡œìš´ ì¸ì—°ì˜ ê¸°ì¨ But, ëª¨ë“  ì‚¬ëŒì´ ê·¸ë ‡ì§€ëŠ” ì•Šë„¤..\n",
       "Name: document, dtype: object"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ë¦¬ë·° ë¬¸ì¥ ì¶”ì¶œ\n",
    "sentences = test['document']\n",
    "sentences[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS] êµ³ ã…‹ [SEP]',\n",
       " '[CLS] GDNTOPCLASSINTHECLUB [SEP]',\n",
       " '[CLS] ë­ì•¼ ì´ í‰ì ë“¤ì€.... ë‚˜ì˜ì§„ ì•Šì§€ë§Œ 10ì  ì§œë¦¬ëŠ” ë”ë”ìš± ì•„ë‹ˆì–ì•„ [SEP]',\n",
       " '[CLS] ì§€ë£¨í•˜ì§€ëŠ” ì•Šì€ë° ì™„ì „ ë§‰ì¥ì„... ëˆì£¼ê³  ë³´ê¸°ì—ëŠ”.... [SEP]',\n",
       " '[CLS] 3Dë§Œ ì•„ë‹ˆì—ˆì–´ë„ ë³„ ë‹¤ì„¯ ê°œ ì¤¬ì„í…ë°.. ì™œ 3Dë¡œ ë‚˜ì™€ì„œ ì œ ì‹¬ê¸°ë¥¼ ë¶ˆí¸í•˜ê²Œ í•˜ì£ ?? [SEP]',\n",
       " '[CLS] ìŒì•…ì´ ì£¼ê°€ ëœ, ìµœê³ ì˜ ìŒì•…ì˜í™” [SEP]',\n",
       " '[CLS] ì§„ì •í•œ ì“°ë ˆê¸° [SEP]',\n",
       " '[CLS] ë§ˆì¹˜ ë¯¸êµ­ì• ë‹ˆì—ì„œ íŠ€ì–´ë‚˜ì˜¨ë“¯í•œ ì°½ì˜ë ¥ì—†ëŠ” ë¡œë´‡ë””ìì¸ë¶€í„°ê°€,ê³ ê°œë¥¼ ì –ê²Œí•œë‹¤ [SEP]',\n",
       " '[CLS] ê°ˆìˆ˜ë¡ ê°œíŒë˜ê°€ëŠ” ì¤‘êµ­ì˜í™” ìœ ì¹˜í•˜ê³  ë‚´ìš©ì—†ìŒ í¼ì¡ë‹¤ ëë‚¨ ë§ë„ì•ˆë˜ëŠ” ë¬´ê¸°ì— ìœ ì¹˜í•œcgë‚¨ë¬´ ì•„ ê·¸ë¦½ë‹¤ ë™ì‚¬ì„œë…ê°™ì€ ì˜í™”ê°€ ì´ê±´ 3ë¥˜ì•„ë¥˜ì‘ì´ë‹¤ [SEP]',\n",
       " '[CLS] ì´ë³„ì˜ ì•„í””ë’¤ì— ì°¾ì•„ì˜¤ëŠ” ìƒˆë¡œìš´ ì¸ì—°ì˜ ê¸°ì¨ But, ëª¨ë“  ì‚¬ëŒì´ ê·¸ë ‡ì§€ëŠ” ì•Šë„¤.. [SEP]']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BERTì˜ ì…ë ¥ í˜•ì‹ì— ë§ê²Œ ë³€í™˜\n",
    "sentences = [\"[CLS] \" + str(sentence) + \" [SEP]\" for sentence in sentences]\n",
    "sentences[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, ..., 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ë¼ë²¨ ì¶”ì¶œ\n",
    "labels = test['label'].values\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] êµ³ ã…‹ [SEP]\n",
      "['[CLS]', 'êµ³', '[UNK]', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "# BERTì˜ í† í¬ë‚˜ì´ì €ë¡œ ë¬¸ì¥ì„ í† í°ìœ¼ë¡œ ë¶„ë¦¬\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased', do_lower_case=False)\n",
    "tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
    "\n",
    "print (sentences[0])\n",
    "print (tokenized_texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 101, 8911,  100,  102,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ì…ë ¥ í† í°ì˜ ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´\n",
    "MAX_LEN = 128\n",
    "\n",
    "# í† í°ì„ ìˆ«ì ì¸ë±ìŠ¤ë¡œ ë³€í™˜\n",
    "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
    "\n",
    "# ë¬¸ì¥ì„ MAX_LEN ê¸¸ì´ì— ë§ê²Œ ìë¥´ê³ , ëª¨ìë€ ë¶€ë¶„ì„ íŒ¨ë”© 0ìœ¼ë¡œ ì±„ì›€\n",
    "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "\n",
    "input_ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "# ì–´í…ì…˜ ë§ˆìŠ¤í¬ ì´ˆê¸°í™”\n",
    "attention_masks = []\n",
    "\n",
    "# ì–´í…ì…˜ ë§ˆìŠ¤í¬ë¥¼ íŒ¨ë”©ì´ ì•„ë‹ˆë©´ 1, íŒ¨ë”©ì´ë©´ 0ìœ¼ë¡œ ì„¤ì •\n",
    "# íŒ¨ë”© ë¶€ë¶„ì€ BERT ëª¨ë¸ì—ì„œ ì–´í…ì…˜ì„ ìˆ˜í–‰í•˜ì§€ ì•Šì•„ ì†ë„ í–¥ìƒ\n",
    "for seq in input_ids:\n",
    "    seq_mask = [float(i>0) for i in seq]\n",
    "    attention_masks.append(seq_mask)\n",
    "\n",
    "print(attention_masks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 101, 8911,  100,  102,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0], dtype=torch.int32)\n",
      "tensor(1)\n",
      "tensor([1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.])\n"
     ]
    }
   ],
   "source": [
    "# ë°ì´í„°ë¥¼ íŒŒì´í† ì¹˜ì˜ í…ì„œë¡œ ë³€í™˜\n",
    "test_inputs = torch.tensor(input_ids)\n",
    "test_labels = torch.tensor(labels)\n",
    "test_masks = torch.tensor(attention_masks)\n",
    "\n",
    "print(test_inputs[0])\n",
    "print(test_labels[0])\n",
    "print(test_masks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°°ì¹˜ ì‚¬ì´ì¦ˆ\n",
    "batch_size = 32\n",
    "\n",
    "# íŒŒì´í† ì¹˜ì˜ DataLoaderë¡œ ì…ë ¥, ë§ˆìŠ¤í¬, ë¼ë²¨ì„ ë¬¶ì–´ ë°ì´í„° ì„¤ì •\n",
    "# í•™ìŠµì‹œ ë°°ì¹˜ ì‚¬ì´ì¦ˆ ë§Œí¼ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜´\n",
    "test_data = TensorDataset(test_inputs, test_masks, test_labels)\n",
    "test_sampler = RandomSampler(test_data)\n",
    "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ëª¨ë¸ ìƒì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GPU ë””ë°”ì´ìŠ¤ ì´ë¦„ ì¶”ì¶œ\n",
    "device_name = tf.test.gpu_device_name()\n",
    "device_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "ename": "SystemError",
     "evalue": "GPU device not found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mSystemError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_6744/3631022097.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Found GPU at: {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mSystemError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'GPU device not found'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mSystemError\u001b[0m: GPU device not found"
     ]
    }
   ],
   "source": [
    "# GPU ë””ë°”ì´ìŠ¤ ì´ë¦„ ê²€ì‚¬\n",
    "if device_name == '/device:GPU:0':\n",
    "    print('Found GPU at: {}'.format(device_name))\n",
    "else:\n",
    "    raise SystemError('GPU device not found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "695b5ead565b4755eca328ffccf87822b806da8ec54984c3001699fc316e428a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('katalkhae': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
